{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2-conv-ai.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0MoaVfWJPr8fC9TSg1X4k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebuehrle/william/blob/main/gpt2_conv_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpS10nvW5uN0",
        "outputId": "4e5b9f40-9938-4f56-b0c2-f9ba1d691209"
      },
      "source": [
        "!git clone https://github.com/huggingface/transfer-learning-conv-ai"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transfer-learning-conv-ai'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Total 132 (delta 0), reused 0 (delta 0), pack-reused 132\u001b[K\n",
            "Receiving objects: 100% (132/132), 56.09 KiB | 2.55 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQwNCQ8l-ziv",
        "outputId": "255ca2e3-2bc2-43fe-b815-b3fb05911b92"
      },
      "source": [
        "%cd transfer-learning-conv-ai/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transfer-learning-conv-ai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXJFGQF1_Q6E",
        "outputId": "f202e26c-a247-49d8-e281-0a8cf191cbc9"
      },
      "source": [
        "!pip install -r requirements.txt -q\n",
        "!python -m spacy download en"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 225kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 13.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 225kB 21.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 20.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 19.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 33.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6MB 40.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.7MB 42.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n",
            "\u001b[31mERROR: botocore 1.20.101 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrU5eM66_Rw0"
      },
      "source": [
        "#!python interact.py"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yncZmtT8t9b"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "from pprint import pformat\n",
        "import torch\n",
        "import random\n",
        "import itertools"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-SodLsAB_4G"
      },
      "source": [
        "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from train import SPECIAL_TOKENS, build_input_from_segments, add_special_tokens_\n",
        "from utils import get_dataset, download_pretrained_model\n",
        "import interact"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsIFLzgJRE7Q"
      },
      "source": [
        "def tokenize(obj, tokenizer):\n",
        "  if isinstance(obj, str):\n",
        "      return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "  if isinstance(obj, dict):\n",
        "      return dict((n, tokenize(o, tokenizer)) for n, o in obj.items())\n",
        "  return list(tokenize(o, tokenizer) for o in obj)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2C03xqlCjLm"
      },
      "source": [
        "def setup_chatbot(personality=None, **kwargs):\n",
        "  args = argparse.Namespace(\n",
        "    dataset_path = kwargs.get(\"dataset_path\", \"\"), # Path or url of the dataset. If empty download from S3.\n",
        "    dataset_cache = kwargs.get(\"dataset_cache\", \"./dataset_cache\"), # Path or url of the dataset cache\n",
        "    model = kwargs.get(\"model\", \"openai-gpt\"), # Model type (openai-gpt or gpt2). Anything besides gpt2 will load openai-gpt.\n",
        "    model_checkpoint = kwargs.get(\"model_checkpoint\", \"\"), # Path, url or short name of the model\n",
        "    max_history = kwargs.get(\"max_history\", 2), # Number of previous utterances to keep in history\n",
        "    device = kwargs.get(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\"), # Device (cuda or cpu)\n",
        "    no_sample = kwargs.get(\"no_sample\", False), # Set to use greedy decoding instead of sampling\n",
        "    max_length = kwargs.get(\"max_length\", 20), # Maximum length of the output utterances\n",
        "    min_length = kwargs.get(\"min_length\", 1), # Minimum length of the output utterances\n",
        "    seed = kwargs.get(\"seed\", 0), # Seed\n",
        "    temperature = kwargs.get(\"temperature\", 0.7), # Sampling softmax temperature\n",
        "    top_k = kwargs.get(\"top_k\", 0), # Filter top-k tokens before sampling (<=0: no filtering)\n",
        "    top_p = kwargs.get(\"top_p\", 0.9), # Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\n",
        "  )\n",
        "\n",
        "  logging.basicConfig(level=logging.INFO)\n",
        "  logger = logging.getLogger()\n",
        "  logger.info(pformat(args))\n",
        "\n",
        "  if args.model_checkpoint == \"\":\n",
        "      if args.model == 'gpt2':\n",
        "          raise ValueError(\"Interacting with GPT2 requires passing a finetuned model_checkpoint\")\n",
        "      else:\n",
        "          args.model_checkpoint = download_pretrained_model()\n",
        "\n",
        "  if args.seed != 0:\n",
        "    random.seed(args.seed)\n",
        "    torch.random.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "  logger.info(\"Get pretrained model and tokenizer\")\n",
        "  tokenizer_class, model_class = (GPT2Tokenizer, GPT2LMHeadModel) if args.model == 'gpt2' else (OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
        "  tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
        "  model = model_class.from_pretrained(args.model_checkpoint)\n",
        "  model.to(args.device)\n",
        "  add_special_tokens_(model, tokenizer)\n",
        "\n",
        "  if personality:\n",
        "    personality = tokenize(personality, tokenizer)\n",
        "  else:\n",
        "    logger.info(\"Sample a personality\")\n",
        "    dataset = get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\n",
        "    personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
        "    personality = random.choice(personalities)\n",
        "\n",
        "  logger.info(\"Selected personality: %s\", tokenizer.decode(itertools.chain(*personality)))\n",
        "  \n",
        "  def _respond(history):\n",
        "    history = [tokenizer.encode(h) for h in history]\n",
        "    with torch.no_grad():\n",
        "        out_ids = interact.sample_sequence(personality, history, tokenizer, model, args)\n",
        "    response = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "    return response\n",
        "  \n",
        "  return _respond"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoIlczeDvl3_"
      },
      "source": [
        "persona = [\"my name is charles william .\",\n",
        "           \"i am from germany .\",\n",
        "           \"i was born in 1679 .\",\n",
        "           \"my brother died in 1672 .\",\n",
        "           \"i founded the city of karlsruhe .\",\n",
        "           \"i was a politician .\"]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSS4fY2bz811",
        "outputId": "8d6a49da-3288-4ae0-de7a-2ce9a36be780"
      },
      "source": [
        "respond = setup_chatbot(persona)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:Namespace(dataset_cache='./dataset_cache', dataset_path='', device='cpu', max_history=2, max_length=20, min_length=1, model='openai-gpt', model_checkpoint='', no_sample=False, seed=0, temperature=0.7, top_k=0, top_p=0.9)\n",
            "INFO:/content/transfer-learning-conv-ai/utils.py:extracting archive file /root/.cache/torch/transformers/2f5114b5eb72f9515802779c42c1b289bebdb1cfc8ce94c653237518eb530b34.75f2a4fe69178ff43138117a977e107a5fc7d402603a0825a296b531f246b3f2 to temp dir /tmp/tmpcn_1asa_\n",
            "INFO:root:Get pretrained model and tokenizer\n",
            "INFO:transformers.tokenization_utils:Model name '/tmp/tmpcn_1asa_' not found in model shortcut name list (openai-gpt). Assuming '/tmp/tmpcn_1asa_' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "INFO:transformers.tokenization_utils:Didn't find file /tmp/tmpcn_1asa_/special_tokens_map.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils:Didn't find file /tmp/tmpcn_1asa_/tokenizer_config.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils:loading file /tmp/tmpcn_1asa_/vocab.json\n",
            "INFO:transformers.tokenization_utils:loading file /tmp/tmpcn_1asa_/merges.txt\n",
            "INFO:transformers.tokenization_utils:loading file /tmp/tmpcn_1asa_/added_tokens.json\n",
            "INFO:transformers.tokenization_utils:loading file None\n",
            "INFO:transformers.tokenization_utils:loading file None\n",
            "WARNING:transformers.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
            "INFO:transformers.configuration_utils:loading configuration file /tmp/tmpcn_1asa_/config.json\n",
            "INFO:transformers.configuration_utils:Model config OpenAIGPTConfig {\n",
            "  \"afn\": \"gelu\",\n",
            "  \"architectures\": null,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"openai-gpt\",\n",
            "  \"n_ctx\": 512,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 512,\n",
            "  \"n_special\": 5,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 40483\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file /tmp/tmpcn_1asa_/pytorch_model.bin\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in OpenAIGPTLMHeadModel: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
            "INFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\n",
            "INFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\n",
            "INFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\n",
            "INFO:transformers.tokenization_utils:Assigning ['<speaker1>', '<speaker2>'] to the additional_special_tokens key of the tokenizer\n",
            "INFO:root:Selected personality: my name is charles william. i am from germany. i was born in 1679. my brother died in 1672. i founded the city of karlsruhe. i was a politician.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAv683C7ztVC"
      },
      "source": [
        "history = [\"hello how are you?\",\n",
        "           \"i am fine thanks.\",\n",
        "           \"where are you from\"]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqI9f5gT0B2_",
        "outputId": "01641c79-57d8-45eb-8f23-4c99bc860189"
      },
      "source": [
        "response = respond(history)\n",
        "print(response)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "germany.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkWFg9SiXJVp"
      },
      "source": [
        "# REST Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "m-be70IHGPwX",
        "outputId": "232f2c51-fd9f-4196-ad36-ae6c88ff76a8"
      },
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install flask-cors\n",
        "!pip install flask==0.12.2  # Newer versions of flask don't work in Colab\n",
        "                            # See https://github.com/plotly/dash/issues/257"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting flask-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.15.0)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.1.4)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.9->flask-cors) (2.0.1)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-3.0.10\n",
            "Collecting flask==0.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/32/e3597cb19ffffe724ad4bf0beca4153419918e7fa4ba6a34b04ee4da3371/Flask-0.12.2-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.4->flask==0.12.2) (2.0.1)\n",
            "Installing collected packages: flask\n",
            "  Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "Successfully installed flask-0.12.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flask"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhZESPF_XVw8",
        "outputId": "75ada179-4509-4bce-82a0-a093bf0ec0ad"
      },
      "source": [
        "from flask import Flask, Response\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import request\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "run_with_ngrok(app)  # Start ngrok when app is run\n",
        "\n",
        "@app.route(\"/\", methods=['POST'])\n",
        "def hello():\n",
        "  history = request.get_json()\n",
        "  print(history)\n",
        "  response = respond(history)\n",
        "  #response = \"dummy response\"\n",
        "  return response\n",
        "\n",
        "@app.route(\"/ping\")\n",
        "def ping():\n",
        "  return \"hello world\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()  # If address is in use, may need to terminate other sessions:\n",
        "               # Runtime > Manage Sessions > Terminate Other Sessions"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://819c1b1a2f99.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:52:47] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['who are you']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:52:52] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:53:08] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['who are you', \"i'm charles william, how are you today?\", 'how are you']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:53:12] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:53:20] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['who are you', \"i'm charles william, how are you today?\", 'how are you', 'i am doing well, how are you?', 'how old are you']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:53:28] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:53:37] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['who are you', \"i'm charles william, how are you today?\", 'how are you', 'i am doing well, how are you?', 'how old are you', \"i'm 10 years old. do you have siblings?\", 'where are you from']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:53:44] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:54:02] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['who are you', \"i'm charles william, how are you today?\", 'how are you', 'i am doing well, how are you?', 'how old are you', \"i'm 10 years old. do you have siblings?\", 'where are you from', 'i live in germany, do you have siblings?', 'are you an actor']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 13:54:11] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:00:28] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['', 'how are you']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:00:33] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:02:13] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 405 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:02:16] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:02:20] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 405 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:02:24] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:05:29] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 405 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:07:03] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['how are you']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:07:07] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:07:49] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['how are you', 'i am good and how are you?', 'my name is etienne']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:07:51] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:07:57] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['how are you', 'i am good and how are you?', 'my name is etienne', 'what is your name?', 'my name is Joe']\n",
            "['i am good and how are you?', 'my name is etienne', 'what is your name?', 'my name is Joe', 'my name is Joe']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:08:01] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:08:02] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:11:27] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['how are you']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:11:33] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:13:07] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 405 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:14:12] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['how are you']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:14:15] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:20:31] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['how are you', \"i'm doing well. how are you?\", 'what is on tv tonight']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:20:37] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:20:53] \"\u001b[37mOPTIONS / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['how are you', \"i'm doing well. how are you?\", 'what is on tv tonight', 'it is a good show. i hope you like it.', 'i am not sure if I will like it']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Jun/2021 14:21:01] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm-jjeSta9NB"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}